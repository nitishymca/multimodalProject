{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-19T10:49:19.809071Z",
     "iopub.status.busy": "2024-06-19T10:49:19.808720Z",
     "iopub.status.idle": "2024-06-19T10:49:20.832810Z",
     "shell.execute_reply": "2024-06-19T10:49:20.831857Z",
     "shell.execute_reply.started": "2024-06-19T10:49:19.809040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wu3d-dipression-detection/normal.json\n",
      "/kaggle/input/wu3d-dipression-detection/depressed.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T10:45:52.873863Z",
     "iopub.status.busy": "2024-06-19T10:45:52.872930Z",
     "iopub.status.idle": "2024-06-19T10:46:23.198234Z",
     "shell.execute_reply": "2024-06-19T10:46:23.196557Z",
     "shell.execute_reply.started": "2024-06-19T10:45:52.873820Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m     data_depressed_user \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/wu3d-dipression-detection/normal.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file2:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the JSON data into a Python dictionary\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     data_normal_user \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Open the JSON file\n",
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/wu3d-dipression-detection/depressed.json\", \"r\") as json_file:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    data_depressed_user = json.load(json_file)\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/wu3d-dipression-detection/normal.json\", \"r\") as json_file2:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    data_normal_user = json.load(json_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:49:52.955874Z",
     "iopub.status.busy": "2024-06-19T10:49:52.955382Z",
     "iopub.status.idle": "2024-06-19T10:50:05.733263Z",
     "shell.execute_reply": "2024-06-19T10:50:05.732201Z",
     "shell.execute_reply.started": "2024-06-19T10:49:52.955845Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "with open('/kaggle/input/wu3d-dipression-detection/depressed.json') as json_file_dipressUser:\n",
    "    jsondata = json.load(json_file_dipressUser)\n",
    "\n",
    "data_file = open('depressUserConvertFile.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    "for data in jsondata:\n",
    "    if count == 0:\n",
    "        header = data.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "    csv_writer.writerow(data.values())\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:50:05.735216Z",
     "iopub.status.busy": "2024-06-19T10:50:05.734920Z",
     "iopub.status.idle": "2024-06-19T10:50:53.313978Z",
     "shell.execute_reply": "2024-06-19T10:50:53.313163Z",
     "shell.execute_reply.started": "2024-06-19T10:50:05.735191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we convert our json file into csv Normal User\n",
    "with open('/kaggle/input/wu3d-dipression-detection/normal.json') as json_file_normal_User:\n",
    "    jsondata = json.load(json_file_normal_User)\n",
    "\n",
    "data_file = open('normalUserConvert.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    "for data in jsondata:\n",
    "    if count == 0:\n",
    "        header = data.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "    csv_writer.writerow(data.values())\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.202467Z",
     "iopub.status.idle": "2024-06-19T10:46:23.202794Z",
     "shell.execute_reply": "2024-06-19T10:46:23.202649Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.202635Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/kaggle/working/depressUserConvertFile.csv')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "str_dict = df['tweets'][1]\n",
    "dictionary = ast.literal_eval(str_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.204021Z",
     "iopub.status.idle": "2024-06-19T10:46:23.204465Z",
     "shell.execute_reply": "2024-06-19T10:46:23.204272Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.204253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dipression - 1\n",
    "\n",
    "tweet_list = []\n",
    "for i in df['tweets']:\n",
    "    dictionary = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary:\n",
    "        tweet_list.append({'texts':j['tweet_content'],'labels':1})\n",
    "\n",
    "# Length of dipressed Tweets\n",
    "len(tweet_list)\n",
    "\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "# tweet_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.206061Z",
     "iopub.status.idle": "2024-06-19T10:46:23.206403Z",
     "shell.execute_reply": "2024-06-19T10:46:23.206256Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.206242Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df_2 = pd.read_csv('/kaggle/working/normalUserConvert.csv')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "str_dict = df_2['tweets'][0]\n",
    "dictionary_2 = ast.literal_eval(str_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:52:51.330909Z",
     "iopub.status.busy": "2024-06-19T10:52:51.330074Z",
     "iopub.status.idle": "2024-06-19T10:53:02.880664Z",
     "shell.execute_reply": "2024-06-19T10:53:02.879640Z",
     "shell.execute_reply.started": "2024-06-19T10:52:51.330878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nickname</th>\n",
       "      <th>gender</th>\n",
       "      <th>profile</th>\n",
       "      <th>birthday</th>\n",
       "      <th>num_of_follower</th>\n",
       "      <th>num_of_following</th>\n",
       "      <th>all_tweet_count</th>\n",
       "      <th>original_tweet_count</th>\n",
       "      <th>repost_tweet_count</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>多吃一点小糖豆</td>\n",
       "      <td>女</td>\n",
       "      <td>给自己起个小名叫快快</td>\n",
       "      <td>巨蟹座</td>\n",
       "      <td>530</td>\n",
       "      <td>380</td>\n",
       "      <td>368</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "      <td>[{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>铁拳会说话</td>\n",
       "      <td>女</td>\n",
       "      <td>漂泊的灵魂</td>\n",
       "      <td>射手座</td>\n",
       "      <td>137</td>\n",
       "      <td>235</td>\n",
       "      <td>154</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>梦想家脸庞eyti</td>\n",
       "      <td>男</td>\n",
       "      <td>在任何科学上的雏形，都有它双重的形象：胚胎时的丑恶，萌芽时的美丽。</td>\n",
       "      <td>无</td>\n",
       "      <td>75</td>\n",
       "      <td>117</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>小MM-Qun</td>\n",
       "      <td>女</td>\n",
       "      <td>Group</td>\n",
       "      <td>01-01</td>\n",
       "      <td>190</td>\n",
       "      <td>410</td>\n",
       "      <td>1834</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '母親節 準備好了麼～', 'posting_time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Outofcuriosity_</td>\n",
       "      <td>女</td>\n",
       "      <td>This is a blog only written original contents....</td>\n",
       "      <td>无</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         nickname gender  \\\n",
       "0      1          多吃一点小糖豆      女   \n",
       "1      1            铁拳会说话      女   \n",
       "2      0        梦想家脸庞eyti      男   \n",
       "3      0          小MM-Qun      女   \n",
       "4      1  Outofcuriosity_      女   \n",
       "\n",
       "                                             profile birthday  \\\n",
       "0                                         给自己起个小名叫快快      巨蟹座   \n",
       "1                                              漂泊的灵魂      射手座   \n",
       "2                  在任何科学上的雏形，都有它双重的形象：胚胎时的丑恶，萌芽时的美丽。        无   \n",
       "3                                              Group    01-01   \n",
       "4  This is a blog only written original contents....        无   \n",
       "\n",
       "   num_of_follower  num_of_following  all_tweet_count  original_tweet_count  \\\n",
       "0              530               380              368                    28   \n",
       "1              137               235              154                    20   \n",
       "2               75               117               87                    75   \n",
       "3              190               410             1834                    99   \n",
       "4                1                 9                6                    12   \n",
       "\n",
       "   repost_tweet_count                                             tweets  \n",
       "0                  70  [{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...  \n",
       "1                   2  [{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...  \n",
       "2                  10  [{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...  \n",
       "3                   1  [{'tweet_content': '母親節 準備好了麼～', 'posting_time...  \n",
       "4                   0  [{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_normal = pd.read_csv('/kaggle/working/normalUserConvert.csv')\n",
    "df_deep = pd.read_csv('/kaggle/working/depressUserConvertFile.csv')\n",
    "\n",
    "# Randomly select 3000 rows from each DataFrame\n",
    "df_normal = df_normal.sample(n=3000, random_state=42)\n",
    "df_deep = df_deep.sample(n=3000, random_state=42)\n",
    "\n",
    "# Concatenate the sampled DataFrames\n",
    "final_df = pd.concat([df_normal, df_deep])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "final_df = shuffle(final_df, random_state=42)\n",
    "\n",
    "# Reset index after shuffling\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the first few rows of the shuffled DataFrame\n",
    "# print(df_combined_shuffled.head())\n",
    "final_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:53:07.913033Z",
     "iopub.status.busy": "2024-06-19T10:53:07.912632Z",
     "iopub.status.idle": "2024-06-19T10:53:07.927515Z",
     "shell.execute_reply": "2024-06-19T10:53:07.926349Z",
     "shell.execute_reply.started": "2024-06-19T10:53:07.913004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '母親節 準備好了麼～', 'posting_time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      1  [{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...\n",
       "1      1  [{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...\n",
       "2      0  [{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...\n",
       "3      0  [{'tweet_content': '母親節 準備好了麼～', 'posting_time...\n",
       "4      1  [{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df[['label','tweets']]\n",
    "\n",
    "print(len(final_df))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T11:00:19.241974Z",
     "iopub.status.busy": "2024-06-19T11:00:19.241558Z",
     "iopub.status.idle": "2024-06-19T12:20:41.626224Z",
     "shell.execute_reply": "2024-06-19T12:20:41.625239Z",
     "shell.execute_reply.started": "2024-06-19T11:00:19.241941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 38518\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be11aca7aa4c68ad02f26e30b9ab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Accuracy: 0.7554, Train Loss: 0.0707, Validation Accuracy: 0.7883, Validation Loss: 0.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Accuracy: 0.7562, Train Loss: 0.0697, Validation Accuracy: 0.7883, Validation Loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Accuracy: 0.7560, Train Loss: 0.0693, Validation Accuracy: 0.7883, Validation Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Accuracy: 0.7554, Train Loss: 0.0685, Validation Accuracy: 0.7883, Validation Loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Accuracy: 0.7550, Train Loss: 0.0669, Validation Accuracy: 0.7883, Validation Loss: 0.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Accuracy: 0.7535, Train Loss: 0.0629, Validation Accuracy: 0.7900, Validation Loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Accuracy: 0.7704, Train Loss: 0.0595, Validation Accuracy: 0.8133, Validation Loss: 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Accuracy: 0.7946, Train Loss: 0.0576, Validation Accuracy: 0.8000, Validation Loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Accuracy: 0.7979, Train Loss: 0.0558, Validation Accuracy: 0.8100, Validation Loss: 0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Accuracy: 0.8102, Train Loss: 0.0532, Validation Accuracy: 0.7833, Validation Loss: 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Accuracy: 0.8194, Train Loss: 0.0513, Validation Accuracy: 0.8183, Validation Loss: 0.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Accuracy: 0.8275, Train Loss: 0.0496, Validation Accuracy: 0.7767, Validation Loss: 0.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Accuracy: 0.8242, Train Loss: 0.0503, Validation Accuracy: 0.8100, Validation Loss: 0.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Accuracy: 0.8319, Train Loss: 0.0477, Validation Accuracy: 0.8017, Validation Loss: 0.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Accuracy: 0.8369, Train Loss: 0.0464, Validation Accuracy: 0.8017, Validation Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Accuracy: 0.8410, Train Loss: 0.0453, Validation Accuracy: 0.8100, Validation Loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Accuracy: 0.8427, Train Loss: 0.0444, Validation Accuracy: 0.8100, Validation Loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Accuracy: 0.8521, Train Loss: 0.0432, Validation Accuracy: 0.8183, Validation Loss: 0.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Accuracy: 0.8565, Train Loss: 0.0417, Validation Accuracy: 0.7950, Validation Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Accuracy: 0.8577, Train Loss: 0.0407, Validation Accuracy: 0.8017, Validation Loss: 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Accuracy: 0.8690, Train Loss: 0.0399, Validation Accuracy: 0.7983, Validation Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Accuracy: 0.8725, Train Loss: 0.0376, Validation Accuracy: 0.7700, Validation Loss: 0.0837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Accuracy: 0.8802, Train Loss: 0.0362, Validation Accuracy: 0.7467, Validation Loss: 0.1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Accuracy: 0.8858, Train Loss: 0.0339, Validation Accuracy: 0.8083, Validation Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Accuracy: 0.8954, Train Loss: 0.0322, Validation Accuracy: 0.7783, Validation Loss: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Accuracy: 0.9056, Train Loss: 0.0307, Validation Accuracy: 0.7833, Validation Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Accuracy: 0.9117, Train Loss: 0.0276, Validation Accuracy: 0.7700, Validation Loss: 0.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Accuracy: 0.9231, Train Loss: 0.0243, Validation Accuracy: 0.7867, Validation Loss: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Accuracy: 0.9306, Train Loss: 0.0225, Validation Accuracy: 0.7867, Validation Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Accuracy: 0.9290, Train Loss: 0.0230, Validation Accuracy: 0.7600, Validation Loss: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Accuracy: 0.9329, Train Loss: 0.0209, Validation Accuracy: 0.7917, Validation Loss: 0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Accuracy: 0.9394, Train Loss: 0.0203, Validation Accuracy: 0.7600, Validation Loss: 0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Accuracy: 0.9390, Train Loss: 0.0190, Validation Accuracy: 0.7867, Validation Loss: 0.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Accuracy: 0.9387, Train Loss: 0.0194, Validation Accuracy: 0.7750, Validation Loss: 0.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Accuracy: 0.9492, Train Loss: 0.0168, Validation Accuracy: 0.8067, Validation Loss: 0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Accuracy: 0.9500, Train Loss: 0.0168, Validation Accuracy: 0.7567, Validation Loss: 0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Accuracy: 0.9490, Train Loss: 0.0162, Validation Accuracy: 0.7550, Validation Loss: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Accuracy: 0.9525, Train Loss: 0.0163, Validation Accuracy: 0.7633, Validation Loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Accuracy: 0.9581, Train Loss: 0.0144, Validation Accuracy: 0.7733, Validation Loss: 0.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Accuracy: 0.9573, Train Loss: 0.0147, Validation Accuracy: 0.7850, Validation Loss: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Accuracy: 0.9604, Train Loss: 0.0128, Validation Accuracy: 0.7667, Validation Loss: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Accuracy: 0.9635, Train Loss: 0.0124, Validation Accuracy: 0.7833, Validation Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Accuracy: 0.9587, Train Loss: 0.0136, Validation Accuracy: 0.7850, Validation Loss: 0.1509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Accuracy: 0.9615, Train Loss: 0.0139, Validation Accuracy: 0.7233, Validation Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Accuracy: 0.9681, Train Loss: 0.0114, Validation Accuracy: 0.7617, Validation Loss: 0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Accuracy: 0.9640, Train Loss: 0.0121, Validation Accuracy: 0.7633, Validation Loss: 0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Accuracy: 0.9675, Train Loss: 0.0121, Validation Accuracy: 0.7267, Validation Loss: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Accuracy: 0.9671, Train Loss: 0.0114, Validation Accuracy: 0.7783, Validation Loss: 0.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Accuracy: 0.9744, Train Loss: 0.0092, Validation Accuracy: 0.7600, Validation Loss: 0.1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Accuracy: 0.9775, Train Loss: 0.0084, Validation Accuracy: 0.7600, Validation Loss: 0.1425\n",
      "Test Accuracy: 0.7650, Test Loss: 0.1327\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Find the maximum length of the sequences in the dataset\n",
    "max_length = max(len(tokenizer.encode(text)) for text in final_df['tweets'])\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "# Set a maximum length for truncation\n",
    "max_length = min(max_length, 128)  # Limit the maximum length to 128 to conserve memory\n",
    "\n",
    "encoded_data = tokenizer(final_df['tweets'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(encoded_data['input_ids'], final_df['label'], test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Define the data loaders\n",
    "train_dataset = TensorDataset(train_texts, torch.tensor(train_labels))  # Ensure train_labels is converted to tensor\n",
    "val_dataset = TensorDataset(val_texts, torch.tensor(val_labels))  # Ensure val_labels is converted to tensor\n",
    "test_dataset = TensorDataset(test_texts, torch.tensor(test_labels))  # Ensure test_labels is converted to tensor\n",
    "\n",
    "# Define the model architecture\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define data loaders with smaller batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size to 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # Reduced batch size to 8\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # Reduced batch size to 8\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        train_acc = train_accuracy / train_total\n",
    "        progress_bar.set_postfix(train_acc=train_acc, train_loss=train_loss / train_total)\n",
    "    train_accuracy /= train_total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_accuracy += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "    val_accuracy /= val_total\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss / train_total:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss / val_total:.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_accuracy = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        test_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "test_accuracy /= test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss / test_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:20:41.628114Z",
     "iopub.status.busy": "2024-06-19T12:20:41.627827Z",
     "iopub.status.idle": "2024-06-19T12:20:41.832248Z",
     "shell.execute_reply": "2024-06-19T12:20:41.830644Z",
     "shell.execute_reply.started": "2024-06-19T12:20:41.628090Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLNetForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test)\n\u001b[1;32m      5\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLNetForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## XLEXNET 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T12:47:33.968034Z",
     "iopub.status.busy": "2024-06-19T12:47:33.967675Z",
     "iopub.status.idle": "2024-06-19T12:48:50.349925Z",
     "shell.execute_reply": "2024-06-19T12:48:50.349030Z",
     "shell.execute_reply.started": "2024-06-19T12:47:33.968005Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tweet_content': '轉發微博', 'posting_time': '2020-04-23 22:09', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '轉發微博', 'posting_time': '2020-04-19 23:48', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/006CSdXtly1fmogxt6z6gj30j60mtwft.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '张艺兴joker为了肚子穿的宽松 苏州', 'posting_time': '2020-04-19 22:57', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '人活着最重要的就是开心啦 苏州', 'posting_time': '2020-04-15 06:17', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1gdu1j27f7xj21430u07cp.jpg'], 'num_of_likes': 12, 'num_of_forwards': 1, 'num_of_comments': 17, 'tweet_is_original': 'True'}, {'tweet_content': '每天沉迷于自己 苏州', 'posting_time': '2020-04-09 21:42', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 2, 'tweet_is_original': 'True'}, {'tweet_content': '对喜欢的人不说我喜欢你 却说我想你 对想见的人不说我想你 却说要不要一起吃个饭 苏州', 'posting_time': '2020-04-07 22:07', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1gdljwclfpkj20u0140gs9.jpg'], 'num_of_likes': 6, 'num_of_forwards': 0, 'num_of_comments': 9, 'tweet_is_original': 'True'}, {'tweet_content': '温馨提醒 早起请勿踩井盖方的圆的都不行 踩了会不幸运的 苏州', 'posting_time': '2020-03-20 07:43', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1gd01x8k7kfj20u00u0qaw.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 2, 'tweet_is_original': 'True'}, {'tweet_content': '致敬白衣天使', 'posting_time': '2020-03-18 20:35', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '早呀 苏州', 'posting_time': '2020-03-17 07:35', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1gcwksod8cvj20u00u0n4z.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '和他分手后的一个月纪念日 神一般同步的大姨妈和痛经 准时的疼醒 苏州', 'posting_time': '2020-03-15 05:27', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1gcu5vqn4wnj20u0143n29.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 1, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2020-03-03 12:59', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 1, 'tweet_is_original': 'False'}, {'tweet_content': '求助我和男朋友这几天没聊天过节他也只是发一句节日快乐前天把我拉进他和他姐的群里了今晚在群里艾特我让我领红包钱是她姐赢的我该不该去领', 'posting_time': '2020-02-13 21:18', 'posted_picture_url': '无', 'num_of_likes': 2, 'num_of_forwards': 0, 'num_of_comments': 1, 'tweet_is_original': 'True'}, {'tweet_content': '我喜欢的人 即便送我淘宝九块九包邮的礼物 那我也中意', 'posting_time': '2020-02-08 22:49', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1gbpdowaqzzj20u00u0773.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '别等了 雪和他都不会来了', 'posting_time': '2020-01-07 21:59', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1gaocevqdh1j20u00u0au6.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '你的坚强比矫情漂亮', 'posting_time': '2020-01-05 21:06', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1galzn2u3tjj20u00u0k8q.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '心动 一身龙纹西装搭配汉服外披越过千年岁月唱一曲霸王别姬立于四方遥望垓下跨年夜舞台老板重新诠释力拔山兮气盖世的楚霸王', 'posting_time': '2020-01-01 08:41', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '新年的祝福我不发了 我已经把我所有的祝福都发给喜欢的人了', 'posting_time': '2020-01-01 04:40', 'posted_picture_url': '无', 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '你没见过我彻夜难眠睁眼到天亮的时候 没见过我委屈难过到哭的不行的时候 没见过我就像得了抑郁症一样需要你的时候 你没见过我洗澡时看到你信息擦擦手回信息的样子 没见过半夜睡着了听见你信息醒来回你的样子 没见过我因为不意间看见你跟别人有关的东西时浑身发抖的样子 没见过我用尽全力维护你时的样子 你也没心疼过我因为你所承受不能承受的 你说你没错那可能是我错了吧 那我放过你吧也放过我自己', 'posting_time': '2020-04-19 07:08', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这就是爱吗爆火不怕你不爱我只怕你以为爱我 01 你确定这就是爱吗真的爱我吗 相信这是大多数人在爱一个人的时候都想知道的答案 所以才有了这首这就是爱吗的爆火 不但旋律动听更有娓娓道出了爱情的一些真相 很多时候你以为的爱其实并不是爱 特别是翻开这首歌的评论更让我们体会到爱一个人的心酸和无奈 你确定这就是爱吗真的爱我吗手牵着手漫步斜阳就当作浪漫 所有细节都在说着他不爱你可他有一张爱你的嘴 缘来you吖 你要知道细节打败爱情那个只嘴上说爱你的人一定不爱你 两个人眺望远方以为爱的晴朗当我回头望却已湿了眼眶 也许这就是爱情最初红了脸现在红了眼我们都很倔强一个不会挽留一个不会回头零度 像我这么骄傲的人一生中输得最惨的那一次是因为你 我不怕你爱不爱我只害怕你以为爱我抓紧我不算拥有你总学不会放手 原来以为年轻不懂什么是爱其实到现在也不懂但是懂得了放手懂得了成全 热评 一个人成全好在两个人的痛苦有一种爱叫做放手 我不怕你不懂爱我只怕你把习惯当作爱你猜不透我要什么 从前从前总以为爱就是一切后来才知道没有例外都会离开北栀180129 原来不爱的理由大家都一样 都说人到了某个年纪最害怕突然听懂一首歌 其实也不是害怕听懂某首歌真正害怕的是这首歌唱的是自己 02 就像歌中唱的那样我不怕你不爱我就怕你以为爱我 感情里最害怕的就是一厢情愿和我以为 一厢情愿地付出奋不顾身地去爱自欺欺人地以为他也爱你 但你知道吗很多时候你以为的喜欢和爱对他来说不是爱而是一种负累 就像那天在D音看到的一个视频 视频里的女生各方面条件都很优秀但她男朋友的条件只是一般般 很多人都不明白为什么这么优秀的她会找一个各方面都不如自己的人 一开始女生以为那是因为男朋友爱她比任何人都爱她而且只爱她 她自欺欺人地骗着所有人说那就是爱包括她自己 所以后来即使他们之间已经没有了感情她自己也爱得满身疲惫但还是不肯放手 朋友劝她分手她说爱了那么久都习惯了 她自己也问男朋友你真的确定你爱我吗 男朋友犹豫了很久才说其实他也不知道只是习惯了她的存在 在他犹豫的瞬间答案已经很明显了 他们在一起好几年时间越久各自对对方就越随意越冷淡 没有了当初的热情更失去了对未来的期待 不分开不是因为爱是她错把习惯当作爱 张小娴在请至少爱一个像男人的男人这本书里说过一段话 走吧有些等待只是一厢情愿归途上只有你自己人家都没留你吃饭再不回去就晚了 不爱了也请分手吧你苦苦哀求他留下的样子真的很难看 他都不爱你了你要给自己留些尊严和体面 03 不属于你的人别留恋不属于你的情别沦陷 上天不给你的你再怎么十指紧扣最终还是会走漏 网上一直流传着一个段子说的是飞蛾和灯泡的故事 灯泡灭了我仔细检查了一下钨丝没有断 我重新按下开关灯泡闪了两下又灭了 我问它你怎么了不开心吗 灯泡回答等会儿有个飞蛾在窗户外面看我好久了 我说那不挺好的终于有人看上你了 灯泡说我不是火别让她看错了误了她一辈子 这长长短短的一生我们或许会遇见很多人但不会跟所有人都有结果 有些遇见注定是用来错过的有些爱注定是没结果的 余生还很长你要明白双向的奔赴才有意义 你可以不远千里穿越人海去见他 但你要知道只有他也想见你的时候你们的见面才有意义爱也一样 不是双向的奔赴所有的用力都只是徒劳 一扇不愿开的门一直敲就不礼貌了 特别想见却一直见不到的人就别见了去等待那个特别想见你的人 等他越过山丘手捧星光来到你面前对你说 嘿久等了余生请多多指教 文程一amp云晞 来源程一djchengyi', 'posting_time': '2020-01-16 17:03', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 1, 'tweet_is_original': 'True'}, {'tweet_content': '接力 ①什么时候开始追星14 ②初恋追星的爱豆是谁虽然很不想承认但就是金恩圣 ③对白嫖什么看法没想法看法好久没追爱豆了 ④本命是谁艺兴 ⑤见过自己本命吗 ⑥在饭圈什么地位没地位 ⑦饭圈人际关系怎么样其实我没圈子 ⑧愿意为蒸煮花吗这个我觉得我这个穷沙雕不配有爱豆 ⑨有墙头吗这个是啥 ⑩艾特你的人和你关系怎么样我也不记得了 ⑪艾特你饭圈姐妹十位回答问题 都说了圈子特别特别特别小了还十个人不好意思了我也是闲来无事就艾特你们了 苏州', 'posting_time': '2020-03-02 18:27', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413egy1gcfr993d3oj20k00jzmy7.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这是我的小小2020届时请多多关照', 'posting_time': '2019-12-31 19:57:14', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1gag5k12j09j20je0ek76h.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '负债', 'posting_time': '2019-12-14 20:05:13', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/a716fd45ly1g9vitfkbv5j20go0tbtnq.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '身材太好了叭', 'posting_time': '2019-12-14 15:12:16', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/a157f83bgy1g9w1pslhhzj21d80wub29.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '早安☀️', 'posting_time': '2019-12-12 08:25:49', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g9tmsglqtbj20u04fd1kx.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '早上好☀️', 'posting_time': '2019-12-08 07:48:20', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g9oz8dpqtjj20u01401kx.jpg'], 'num_of_likes': 2, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '❄ 十二月它来了', 'posting_time': '2019-12-01 09:29:41', 'posted_picture_url': '无', 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我在等级达到LV6获得活跃粉丝头衔小伙伴们快来加入张艺兴超话一起聊聊吧张艺兴', 'posting_time': '2019-11-27 11:05:29', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '需要的时候我都在❤', 'posting_time': '2019-11-22 10:06:30', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g96lb72095j20pz10cwre.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '靓女笑而不语', 'posting_time': '2019-11-21 11:44:51', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '朋友家山西运城的她家的冰糖心苹果很好吃需要的可以随时私信我', 'posting_time': '2019-11-19 13:59:09', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这姑娘可真俊', 'posting_time': '2019-11-18 16:58:27', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '人都有傻的时候 而我是一直傻', 'posting_time': '2019-11-18 07:54:25', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g91v094v3gj20q20wndk9.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '安全第一哥哥我们等着二巡郑州站', 'posting_time': '2019-11-16 14:36:56', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '希望活动一切顺利大家注意安全也注意公共场合的秩序', 'posting_time': '2019-11-16 11:52:08', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/e9f5902dgy1g8zp098ldtj20c80dpdgu.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2019-11-16 08:33:58', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '感谢对我的信任☺ ❤ 郑州', 'posting_time': '2019-11-16 08:27:25', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g8zkps46mlj20u00u0tbb.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我又双叒叕来卖苹果了 不厌其烦的介绍一下 冰糖心', 'posting_time': '2019-11-13 10:15:46', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g8w703x14vj20u01hc1kx.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这个冬天来了', 'posting_time': '2019-11-09 09:10:46', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '别犯傻 也别以为你一直付出零碎的东西 就可以赢了 起码要懂得怎样做才最好', 'posting_time': '2019-11-08 11:50:09', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8qhmkw401j20pz1acq8n.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '有哪些能让你一秒消除负能量的句子 01 假如生活欺骗了你 不要着急 拿出美颜相机去欺骗生活 02 不要垂头丧气了 显矮 03 既然喝水都胖 那干嘛不喝可乐 04 不知道你们都是混什么圈子的 我混的是黑眼圈 05 虽然我不会做饭 但我点的一手好外卖啊 06 只要是能用钱解决的问题 我一件都解决不了 07 虽然你已经无法再长高 但是你可以继续胖啊 08 人的想法是会变的 以前我也是想致富的 但现在我只想脱贫 09 我有一堆问题 一堆麻烦 还有一堆肉 10 上天给了我很多变胖的机会 我都成功抓住了 11 想到明天还有很多事做 我直接睡到后天 12 想我呢就来找我 让我打喷嚏算怎么回事 13 如果不能一夜暴富 两夜也行 半个月我也可以接受 14 道理我都懂 可是听到别人喊美女的时候 还是忍不住回头 15 有时候挺讨厌自己的 不会说话太过善良没什么心眼 还这么可爱 16 这世界最美好的事情 莫过于吃肉 从来不会背叛从来不会欺骗 吃一斤长一斤 永远真诚相待 17 多年前你的一句保重 我至今没瘦 18 我没什么野心 只是单纯的想发财而已 19 靠近我之前你要考虑清楚 除了美貌我一无所有 20 处对象吗 你喜欢什么我给你买 但是超过三块钱的就算了 我不喜欢物质的男孩 21 我饿了 果然诚实可靠幽默风趣温柔善良体贴 可爱单纯天真浪漫大方有趣优雅美丽 不能当饭吃', 'posting_time': '2019-11-29 20:12:15', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我在等级达到LV5获得进阶粉丝头衔小伙伴们快来加入张艺兴超话一起聊聊吧张艺兴', 'posting_time': '2019-11-08 10:07:07', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '买苹果吗 又大又甜 又新鲜 还很脆的那种', 'posting_time': '2019-11-08 10:05:45', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8qelv65hlj208c0b40um.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '为了人间烟火气儿 早ﾉ☀', 'posting_time': '2019-11-07 07:49:50', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8p52dq1hqj20u01hcjyg.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我一直都在啊 需要了来找我吧', 'posting_time': '2019-11-06 17:48:06', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g8ogq5kogsj20u01404qp.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '不仅是自己的生日礼物还是送粉丝的凡凡实力宠粉 吴亦凡新歌贰叁是送给自己最好的生日礼物吗', 'posting_time': '2019-11-06 11:08:37', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我在等级达到LV4获得进阶粉丝头衔小伙伴们快来加入张艺兴超话一起聊聊吧张艺兴', 'posting_time': '2019-11-06 10:52:47', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '苹果走一箱 抓住机会带走实惠 10斤36还包邮 关键是地里现摘的', 'posting_time': '2019-11-05 14:02:41', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8n4kr9ehuj20pw1bz193.jpg'], 'num_of_likes': 2, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '好怀念', 'posting_time': '2019-11-04 15:12:12', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '一直觉得自己颧骨高', 'posting_time': '2019-11-04 13:06:07', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g8lxa07f6uj20u0140gqf.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 6, 'tweet_is_original': 'True'}, {'tweet_content': '已帅晕过去', 'posting_time': '2019-11-04 10:43:23', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8lt7sp0xuj20u0190aje.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这是山西运城临猗县的苹果冰糖心的', 'posting_time': '2019-11-04 10:14:45', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '我的力量很微薄今年猪肉太贵了苹果不能放冷藏只能往外售完但是快递费比苹果本身还贵', 'posting_time': '2019-11-04 10:13:56', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 1, 'num_of_comments': 1, 'tweet_is_original': 'True'}, {'tweet_content': '特价特价特价 苹果都是现摘现卖 包邮包邮包邮 重要的事情说三遍 新郑市', 'posting_time': '2019-11-04 09:56:14', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8lrv0t0mmj20pe0xun00.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '愿你在这个冬天 有衣暖身 有人暖心❤', 'posting_time': '2019-11-03 19:12:18', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8l2b1xaomj20u0140gwb.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '原地晕倒', 'posting_time': '2019-11-03 16:49:21', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/d60fbcc9gy1g8knjohevtj23n83n8he2.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '分享一下 红苹果实物图 现摘现卖 纯天然无公害', 'posting_time': '2019-11-03 10:18:11', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8kmvjoymtj20u0140kbi.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '上班了', 'posting_time': '2019-11-03 09:46:32', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这人间本不该令我这么欣喜的 但是你来了', 'posting_time': '2019-11-03 06:59:40', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8kh4p9f0fj20u06wk7wh.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '早安 十一月了 年前最后的冲刺 你们准备好了嘛', 'posting_time': '2019-11-01 07:54:51', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8i7hfip06j20u014078y.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 2, 'tweet_is_original': 'True'}, {'tweet_content': '回郑州', 'posting_time': '2019-10-31 15:05:20', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8heaz53t0j20u00u0te2.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '一个苹果一斤四两 这也太大啦 一个人都吃不完', 'posting_time': '2019-10-31 09:38:55', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8h4vhhhypj20u014042i.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '早ﾉ☀', 'posting_time': '2019-10-31 08:25:56', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8h2rtt7uyj20u01hctjl.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '吴乾穿白大褂太帅了', 'posting_time': '2019-10-30 19:10:06', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8gfrci2d3j21hc0u07wh.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我关注了黄子韬超话小伙伴们快来加入黄子韬超话一起聊聊吧戳这里→ 黄子韬', 'posting_time': '2019-10-30 18:35:49', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '晒太阳☀ 最后在家悠闲一天了 明天就去上班咯', 'posting_time': '2019-10-30 10:18:57', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8g0eow5d1j20u00u0gog.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我们都越来越好啊', 'posting_time': '2019-10-29 23:15:20', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8fh8ir2ooj20pw0qywig.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 4, 'tweet_is_original': 'True'}, {'tweet_content': '最后院长告诉我 喜欢一个人不能把他推开 很感谢院长和帅帅的贺医生', 'posting_time': '2019-10-29 14:03:35', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8f19jwzurj20u00u0djn.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '视力恢复的好多了 西安', 'posting_time': '2019-10-29 12:21:56', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8eycq94hlj20u00u0769.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '如果你觉得人生太难就去医院看一看吧 西安', 'posting_time': '2019-10-29 09:24:11', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8et3jqfu3j20u00u00yh.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '晚安✨', 'posting_time': '2019-10-28 22:44:31', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8eaq99w80j21430u078h.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '回头客 多谢支持', 'posting_time': '2019-10-27 17:42:23', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g8cwd32siej20qf1cq0x9.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '出来晒个太阳', 'posting_time': '2019-10-27 14:56:17', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '苹果走一箱✌', 'posting_time': '2019-10-27 14:34:07', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g8cqwutd9jj20q31chq72.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '冬季干燥易上火没事来一个香甜渴口的苹果好吃美容补充维生素效果更好山西临猗红富士冰糖心苹果10斤装需要的赶紧私信我☺ ☺ ☺', 'posting_time': '2019-10-27 13:07:13', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g8coeq4t6tj20u01407a0.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '自家种的冰糖心苹果 需要的可以微我 18338800303 电话也可以订', 'posting_time': '2019-10-23 18:30:53', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g88baar2qzj20u0140age.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '腿太长 裤子太短', 'posting_time': '2019-10-23 09:11:14', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g87v4hgn2xj20u00u0jxt.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 6, 'tweet_is_original': 'True'}, {'tweet_content': '其实人生没有什么难的 只要你努力 其实大家过得都累都有难处 像我年纪轻轻就住进了老年人的病区 你看大家是不是都比你难 西安', 'posting_time': '2019-10-16 07:41:55', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/bcf2413ely1g7zp49aqo5j20u01hce81.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '保温杯里泡枸杞 以后进入养生状态', 'posting_time': '2019-10-11 16:34:34', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1g7uchrq5kgj22bs2bsqsi.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '早上好啊 天气晴朗☀', 'posting_time': '2019-10-10 08:50:50', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g7stgl7o63j22bs2bshdt.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '我关注了随手拍超话小伙伴们快来加入随手拍超话一起聊聊吧戳这里→ 随手拍', 'posting_time': '2019-10-07 15:40:40', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '对于我来说也是新的开始', 'posting_time': '2019-10-07 06:28:58', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/a157f83bly3g7oxg9fpu7j20u00gv45h.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '羡煞众人', 'posting_time': '2019-10-02 08:51:07', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/a716fd45ly1g7j1pkuh3cg20g2090hdw.gif'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '阿中哥哥太帅了 无论何时 无论何地 我爱你 我的祖国 阜阳', 'posting_time': '2019-10-02 06:10:37', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1g7jfvelt92j20ia0c6747.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '骄傲感动 祖国 我为你骄傲', 'posting_time': '2019-10-02 05:50:21', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '祖国 我为你骄傲', 'posting_time': '2019-10-02 05:50:02', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '为我的祖国庆生', 'posting_time': '2019-10-01 03:33:36', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413ely1g7i5q0ircwj20ia0c6wef.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2019-08-16 10:29:23', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/6d2acb3agy1g5zoaq3a1bj20c874g4nq.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '网友安利的可可爱爱的生日文案马住 1今天有人过生日吗如果没有我过 2按时长大叮 3ʜᴀ͟ᴘ͟ᴘ͟ʏ ᴇᴠᴇʀʏᴅᴀʏ̆̈ 4♡ ʜᴀᴘᴘʏ ʙɪʀᴛʜᴅᴀʏ ᴛᴏ ᴍᴇ ◟̆◞̆ ♡ 5今天生日 帮忙许愿 一条五元 6十八一轮回 今天我五岁 7和爸妈认识的第n年 8祝我 八岁的十周年纪念日快乐 9生活明朗 万物可爱 我按时长大 10Level  1 升级打怪 11打扰一下 这个美女在线等待生日祝福 12XX来到地球观察人类的第XX年 13观察世界的第xx年 作者xxx 14成为地球观察员的第18年 15对于xx岁的你的祝福 是经历世事而不失少年意趣 是保持坚定与热爱 是依然能够为世间那些真心实意而心动 16人生进度条 ▓▓▓░░░░░░░░░ 18 17来干了这碗二十一年的人间烟火多少岁就说多少年 18愿 每一岁都能奔走在自己的热爱里 19今天又与这值得的人间多相处了一年 20入驻世界第xx个年头 21无论多大 都要热爱童话 英雄和魔法 22离退休又近了一年～ 23今天的星星都在我眼里抱歉啦各位 已经可爱了18年啦 希望我把甜甜的奶油抹脸上 写给月亮的诗放心里 24真的不和我说生日快乐吗不然下一次要再365天后 25要陪在喜欢的人身边一年又一年 26世间唯一不需要努力就能得到的是年龄××岁 27永远二十赶朝暮', 'posting_time': '2019-08-09 11:14:05', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/e72082c6gy1g5sevln3sxj20j60enq47.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2019-07-22 11:23:16', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2019-07-13 12:01:44', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '今天是我的生日06月07日来祝福我吧', 'posting_time': '2019-06-07 01:59:44', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2019-02-05 09:06:17', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/006WGDZaly1fzuri08vuuj30go0b474r.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2019-02-04 22:15:40', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2019-01-30 23:36:17', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/d2e6601cgy1fqkon4o5emj20j60y3tda.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '不不不张艺兴不是单身他女朋友在这里', 'posting_time': '2019-01-13 08:07:56', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2019-01-09 08:38:00', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '绅士兴❤', 'posting_time': '2019-01-06 09:29:45', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/ab9754c5ly1fyumntze5wj20c80figmf.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '哥休息多了咋赚钱咋看你呢', 'posting_time': '2019-01-02 11:12:13', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-12-24 08:27:42', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/e72082c6gy1fyfeuilxpsj208107i0sx.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-12-21 08:56:49', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-12-18 00:20:28', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-12-15 20:24:23', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/adaf08e3ly1fy5fwpwuxjj20rs4j6b2d.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '难为我艺兴了', 'posting_time': '2018-12-09 15:32:20', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '太可爱了吧', 'posting_time': '2018-12-09 10:04:30', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-11-25 07:33:17', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/005Hvwhxly1fxhybgpj5jj30yi196gst.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '中国一点都不能少 中国一点都不能少 中国一点都不能少', 'posting_time': '2018-11-18 10:41:43', 'posted_picture_url': ['http://ww3.sinaimg.cn/wap180/eaaf2affgw1f5pwuzt0wmj20en0m80u0.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '希望你以后的路再难走都熬过 别把难过留给别人看笑话 以后越来越成熟 做自己想做的事', 'posting_time': '2018-11-18 10:37:38', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/bcf2413egy1fxc0jag8szj20qo0qojvb.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 1, 'tweet_is_original': 'True'}, {'tweet_content': '❤️秒变杨幂同款大长腿❗️ 5个动作轻松改善骨盆前倾❗️超燃动作轻松瘦大腿瘦小肚子❗️轻松练出铅笔腿翘翘臀❗️❤️ ⚠️骨盆前倾导致小肚子突出大腿粗壮臀部大体态老怎么办呢 ✅什么是骨盆前倾骨盆前倾的表现特点 骨盆前倾是骨盆位置偏移的病态现象较正确的的骨盆位置向前倾斜有一定的角度骨盆前倾最明显的症状就是臀部后凸腰臀比BMI值都在正常范围小腹仍然前凸 ✅如何科学的自测 找一面墙身体贴紧墙面胸椎和骶骨贴紧墙面一只手放于腰间测试腰间距离中间要是手掌️距离那就是正常的要是可以放进一只拳头的距离那就是骨盆前倾了 上面我们了解了骨盆前倾的原因和自测方法 那么下面我就教大家五个动作轻松改善骨盆前倾～ ❤️动作一仰卧足踝屈伸 动作讲解双腿弯曲仰卧躺下将弹力带绕于左脚长双手握住弹力带两端大臂自然放于身体两侧贴实地面 左腿伸直与地面垂直弹力带保持张力踝关节交替屈伸每侧15次做34组 ❤️动作二仰卧腿部拉伸 动作讲解在上一个动作基础上臀部带动腿部顺时针转动10次在逆时针旋转10每侧腿做34组 ❤️动作三变式臀桥静态动态均可 动作讲解仰卧躺下双手放于体侧膝盖弯曲脚尖点地弹力带放在膝盖上骨盆后倾臀部发力一节一节抬起脊椎每次做34组每组30秒～ ❤️动作四蚌式开合进阶 动作讲解屈膝侧躺下弹力带放于膝盖上侧慢慢抬起腿臀部发力最高点停3秒后慢慢落下不要落到最低每侧15次做34组 ❤️动作五 仰卧臀部拉伸 动作讲解屈膝仰卧躺下左小腿水平放于右大腿上双腿交叉将右大腿拉向胸部腰和背部紧贴地面每侧做10次做34组～ 上面就是可以轻松改善骨盆前倾的五个动作了你们学会了吗希望每个妹子练习后都可以轻松减掉粗壮的大腿和突出的小腹恢复A4腰和漫画腿哦加油', 'posting_time': '2019-07-13 12:01:58', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/d9906e1agy1g3zl7iz6sjj20ku0rsdkv.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '这位先生请注意您的自拍姿势和角度', 'posting_time': '2018-11-16 21:28:19', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/a157f83bgy1fx9eged9t5j20qo0zidq7.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '谁向谁妥协 谁跟谁道别', 'posting_time': '2018-11-15 17:11:09', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1fx8uzzz801j20ni0eodh6.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2018-11-15 16:56:30', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-11-05 16:37:20', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/005Lolf1ly1ftrrwdb895j30hs1nvn6f.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-10-28 10:58:17', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-10-24 15:23:11', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-10-22 17:27:06', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-10-13 22:50:56', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-10-13 22:38:52', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '生日快乐呀～三胎真的很棒期待四胎五胎', 'posting_time': '2018-10-07 20:32:47', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '生日快乐呀～', 'posting_time': '2018-10-07 20:31:44', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '来爱丽们的怀里来', 'posting_time': '2018-10-02 09:16:29', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/bcf2413ely1fvtm30i638j20lx1560u1.jpg'], 'num_of_likes': 1, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '祝愿我国国富民强', 'posting_time': '2018-10-01 17:42:50', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-08-24 23:13:02', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/b1d1fcb0gy1ful676nr0xj20iw3dwdps.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-06-18 22:55:59', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-05-17 18:49:07', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/b179e898gy1fre2d324ngj20z71qg7wi.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-05-17 10:17:20', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-05-07 18:06:35', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '晚安', 'posting_time': '2018-04-25 22:26:02', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/bcf2413ely1fqp9qzznh1j20u00u07io.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2018-04-22 11:01:14', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/005v4nJEly1fqkb91gk46j30he0f3n8m.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-22 10:59:47', 'posted_picture_url': ['http://wx4.sinaimg.cn/wap180/7186f99agy1fqk756byakj20ur0urgw9.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-17 22:12:26', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-17 22:06:45', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-09 08:01:47', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/006d3jPagy1fpmi0g11lxj30c80c8jrv.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-07 23:38:24', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-07 17:39:12', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/9f26e35dgy1fnirgngtfaj20fa0p0dkc.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-04-06 23:03:32', 'posted_picture_url': ['http://wx3.sinaimg.cn/wap180/006KIozegy1fpyljoomtoj30hs0u0jv3.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '分享我的故事 微博故事', 'posting_time': '2018-04-05 07:45:01', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'True'}, {'tweet_content': '无', 'posting_time': '2018-01-28 18:48:57', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2018-01-24 10:33:47', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '', 'posting_time': '2017-12-22 18:56:09', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '有点想哭终于见面了', 'posting_time': '2017-12-14 19:50:01', 'posted_picture_url': ['http://wx2.sinaimg.cn/wap180/a157f83bly1fmgcsodshuj21be0qo0x4.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2017-12-10 19:05:43', 'posted_picture_url': ['http://wx1.sinaimg.cn/wap180/b0a3cc0dgy1fmbhhmm8dgj217r1ion3e.jpg'], 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '为你唱首生日歌祝你永远18岁 查看动图', 'posting_time': '2017-11-06 08:13:00', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2017-10-16 13:13:35', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2017-10-16 13:13:31', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}, {'tweet_content': '无', 'posting_time': '2017-10-16 13:13:26', 'posted_picture_url': '无', 'num_of_likes': 0, 'num_of_forwards': 0, 'num_of_comments': 0, 'tweet_is_original': 'False'}]\n",
      "1497125\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/kaggle/working/depressUserConvertFile.csv')\n",
    "\n",
    "str_dict = df['tweets'][1]\n",
    "dictionary = ast.literal_eval(str_dict)\n",
    "\n",
    "# dictionary\n",
    "print(dictionary)\n",
    "\n",
    "tweet_list_1 = []\n",
    "for i in df['tweets']:\n",
    "    dictionary = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary:\n",
    "        # tweet_list_1.append({'posting time':j['posting_time'],'labels':1})\n",
    "        tweet_list_1.append({'posting time':j['posting_time'],'orignal tweet':j['tweet_is_original'], 'labels':1})\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "print(len(tweet_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:48:50.352546Z",
     "iopub.status.busy": "2024-06-19T12:48:50.352075Z",
     "iopub.status.idle": "2024-06-19T12:49:18.918489Z",
     "shell.execute_reply": "2024-06-19T12:49:18.917572Z",
     "shell.execute_reply.started": "2024-06-19T12:48:50.352513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454300\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df_2 = pd.read_csv('/kaggle/working/normalUserConvert.csv')\n",
    "\n",
    "str_dict = df_2['tweets'][0]\n",
    "dictionary_2 = ast.literal_eval(str_dict)\n",
    "\n",
    "tweet_list_normal = []\n",
    "for i in df['tweets']:\n",
    "    dictionary_2 = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary_2:\n",
    "        tweet_list_normal.append({'Post Timing':j['posting_time'], 'orignal tweet':j['tweet_is_original'], 'labels':0})\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "print(len(tweet_list_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:49:18.920277Z",
     "iopub.status.busy": "2024-06-19T12:49:18.919906Z",
     "iopub.status.idle": "2024-06-19T12:49:21.993586Z",
     "shell.execute_reply": "2024-06-19T12:49:21.992598Z",
     "shell.execute_reply.started": "2024-06-19T12:49:18.920245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>Post Timing</th>\n",
       "      <th>orignal tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-16 21:06:10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-29 22:51:59</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-19 23:51:27</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels          Post Timing orignal tweet\n",
       "0       0  2019-09-16 21:06:10          True\n",
       "1       1                  NaN          True\n",
       "2       0  2019-09-29 22:51:59          True\n",
       "3       0  2019-09-19 23:51:27          True\n",
       "4       1                  NaN         False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the tweet_list_normal and tweet_list_dipressed into DataFrames\n",
    "df_normal_posting = pd.DataFrame(tweet_list_normal)\n",
    "df_deep_posting = pd.DataFrame(tweet_list_1)\n",
    "\n",
    "# Randomly select 3000 rows from each DataFrame\n",
    "df_normal_sampled = df_normal_posting.sample(n=3000, random_state=42)\n",
    "df_deep_sampled = df_deep_posting.sample(n=3000, random_state=42)\n",
    "\n",
    "# Concatenate the sampled DataFrames\n",
    "df_combined = pd.concat([df_normal_sampled, df_deep_sampled])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df_combined_shuffled = shuffle(df_combined, random_state=42)\n",
    "\n",
    "# Reset index after shuffling\n",
    "df_combined_shuffled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the first few rows of the shuffled DataFrame\n",
    "# print(df_combined_shuffled.head())\n",
    "df_combined_shuffled.head(10)\n",
    "\n",
    "final_df = df_combined_shuffled[['labels','Post Timing', 'orignal tweet']]\n",
    "\n",
    "print(len(final_df))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:49:21.996341Z",
     "iopub.status.busy": "2024-06-19T12:49:21.995998Z",
     "iopub.status.idle": "2024-06-19T12:49:22.022429Z",
     "shell.execute_reply": "2024-06-19T12:49:22.021398Z",
     "shell.execute_reply.started": "2024-06-19T12:49:21.996315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Behavior-based Features:\n",
      "   Proportion of Original Tweets  Proportion of Late-night Posts  \\\n",
      "0                       0.978231                        0.264966   \n",
      "\n",
      "   Posting Frequency (per week)  Standard Deviation of Posting Time  \n",
      "0                         514.5                        1.469051e+06  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/2136179146.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['Post Timing'] = pd.to_datetime(final_df['Post Timing'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'Post Timing' column to datetime format, handle parsing errors\n",
    "final_df['Post Timing'] = pd.to_datetime(final_df['Post Timing'], errors='coerce')\n",
    "\n",
    "# Remove rows with NaT values (parsing errors)\n",
    "final_df = final_df.dropna(subset=['Post Timing'])\n",
    "\n",
    "# Calculate features for each label separately\n",
    "social_features = {}\n",
    "\n",
    "# Iterate over unique labels\n",
    "for label in final_df['labels'].unique():\n",
    "    # Subset the DataFrame for the current label\n",
    "    subset_df = final_df[final_df['labels'] == label]\n",
    "\n",
    "    # Calculate features\n",
    "    # Proportion of original tweets\n",
    "    proportion_original = subset_df['orignal tweet'].value_counts(normalize=True)['True']\n",
    "\n",
    "    # Late-night posting (posts made between 11 PM and 6 AM)\n",
    "    late_night_posts = subset_df[(subset_df['Post Timing'].dt.hour >= 23) | (subset_df['Post Timing'].dt.hour <= 6)]\n",
    "    proportion_late_night_posts = len(late_night_posts) / len(subset_df)\n",
    "\n",
    "    # Posting frequency (per week)\n",
    "    posting_frequency_per_week = len(subset_df) / ((subset_df['Post Timing'].max() - subset_df['Post Timing'].min()).days / 7)\n",
    "\n",
    "    # Standard deviation of posting time\n",
    "    std_dev_posting_time = subset_df['Post Timing'].diff().dt.total_seconds().std()\n",
    "\n",
    "    # Store features for the current label\n",
    "    social_features[label] = {\n",
    "        'Proportion of Original Tweets': proportion_original,\n",
    "        'Proportion of Late-night Posts': proportion_late_night_posts,\n",
    "        'Posting Frequency (per week)': posting_frequency_per_week,\n",
    "        'Standard Deviation of Posting Time': std_dev_posting_time\n",
    "    }\n",
    "\n",
    "# Convert features dictionary to DataFrame\n",
    "social_features_df = pd.DataFrame.from_dict(social_features, orient='index')\n",
    "\n",
    "# Display the social features\n",
    "print(\"Social Behavior-based Features:\")\n",
    "print(social_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:49:22.024627Z",
     "iopub.status.busy": "2024-06-19T12:49:22.024273Z",
     "iopub.status.idle": "2024-06-19T13:19:32.516991Z",
     "shell.execute_reply": "2024-06-19T13:19:32.516064Z",
     "shell.execute_reply.started": "2024-06-19T12:49:22.024596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Accuracy: 1.0000, Train Loss: 0.0005, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Accuracy: 1.0000, Train Loss: 0.0000, Validation Accuracy: 1.0000, Validation Loss: 0.0000\n",
      "Test Accuracy: 1.0000, Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "# final_df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Combine 'Post Timing' and 'orignal tweet' features\n",
    "final_df['features'] = final_df['Post Timing'].astype(str) + ' ' + final_df['orignal tweet']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Find the maximum length of the sequences in the dataset\n",
    "max_length = max(len(tokenizer.encode(text)) for text in final_df['features'])\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "# Set a maximum length for truncation\n",
    "max_length = min(max_length, 64)  # Limit the maximum length to 128 to conserve memory\n",
    "\n",
    "encoded_data = tokenizer(final_df['features'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(encoded_data['input_ids'], final_df['labels'], test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Define the data loaders\n",
    "train_dataset = TensorDataset(train_texts, torch.tensor(train_labels))  # Ensure train_labels is converted to tensor\n",
    "val_dataset = TensorDataset(val_texts, torch.tensor(val_labels))  # Ensure val_labels is converted to tensor\n",
    "test_dataset = TensorDataset(test_texts, torch.tensor(test_labels))  # Ensure test_labels is converted to tensor\n",
    "\n",
    "# Define the model architecture\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define data loaders with smaller batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size to 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced batch size to 8\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)  # Reduced batch size to 8\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        train_acc = train_accuracy / train_total\n",
    "        progress_bar.set_postfix(train_acc=train_acc, train_loss=train_loss / train_total)\n",
    "    train_accuracy /= train_total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_accuracy += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "    val_accuracy /= val_total\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss / train_total:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss / val_total:.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_accuracy = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        test_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "test_accuracy /= test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss / test_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T13:19:32.519058Z",
     "iopub.status.busy": "2024-06-19T13:19:32.518395Z",
     "iopub.status.idle": "2024-06-19T13:19:32.557177Z",
     "shell.execute_reply": "2024-06-19T13:19:32.556093Z",
     "shell.execute_reply.started": "2024-06-19T13:19:32.519023Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot loss curves\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T13:19:32.557849Z",
     "iopub.status.idle": "2024-06-19T13:19:32.558206Z",
     "shell.execute_reply": "2024-06-19T13:19:32.558018Z",
     "shell.execute_reply.started": "2024-06-19T13:19:32.558005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4493575,
     "sourceId": 7698496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
