{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-19T10:49:19.809071Z",
     "iopub.status.busy": "2024-06-19T10:49:19.808720Z",
     "iopub.status.idle": "2024-06-19T10:49:20.832810Z",
     "shell.execute_reply": "2024-06-19T10:49:20.831857Z",
     "shell.execute_reply.started": "2024-06-19T10:49:19.809040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wu3d-dipression-detection/normal.json\n",
      "/kaggle/input/wu3d-dipression-detection/depressed.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-19T10:45:52.873863Z",
     "iopub.status.busy": "2024-06-19T10:45:52.872930Z",
     "iopub.status.idle": "2024-06-19T10:46:23.198234Z",
     "shell.execute_reply": "2024-06-19T10:46:23.196557Z",
     "shell.execute_reply.started": "2024-06-19T10:45:52.873820Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m     data_depressed_user \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/wu3d-dipression-detection/normal.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file2:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the JSON data into a Python dictionary\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     data_normal_user \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Open the JSON file\n",
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/wu3d-dipression-detection/depressed.json\", \"r\") as json_file:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    data_depressed_user = json.load(json_file)\n",
    "\n",
    "\n",
    "with open(\"/kaggle/input/wu3d-dipression-detection/normal.json\", \"r\") as json_file2:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    data_normal_user = json.load(json_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:49:52.955874Z",
     "iopub.status.busy": "2024-06-19T10:49:52.955382Z",
     "iopub.status.idle": "2024-06-19T10:50:05.733263Z",
     "shell.execute_reply": "2024-06-19T10:50:05.732201Z",
     "shell.execute_reply.started": "2024-06-19T10:49:52.955845Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "with open('/kaggle/input/wu3d-dipression-detection/depressed.json') as json_file_dipressUser:\n",
    "    jsondata = json.load(json_file_dipressUser)\n",
    "\n",
    "data_file = open('depressUserConvertFile.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    "for data in jsondata:\n",
    "    if count == 0:\n",
    "        header = data.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "    csv_writer.writerow(data.values())\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:50:05.735216Z",
     "iopub.status.busy": "2024-06-19T10:50:05.734920Z",
     "iopub.status.idle": "2024-06-19T10:50:53.313978Z",
     "shell.execute_reply": "2024-06-19T10:50:53.313163Z",
     "shell.execute_reply.started": "2024-06-19T10:50:05.735191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we convert our json file into csv Normal User\n",
    "with open('/kaggle/input/wu3d-dipression-detection/normal.json') as json_file_normal_User:\n",
    "    jsondata = json.load(json_file_normal_User)\n",
    "\n",
    "data_file = open('normalUserConvert.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    "for data in jsondata:\n",
    "    if count == 0:\n",
    "        header = data.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    "    csv_writer.writerow(data.values())\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.202467Z",
     "iopub.status.idle": "2024-06-19T10:46:23.202794Z",
     "shell.execute_reply": "2024-06-19T10:46:23.202649Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.202635Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/kaggle/working/depressUserConvertFile.csv')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "str_dict = df['tweets'][1]\n",
    "dictionary = ast.literal_eval(str_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.204021Z",
     "iopub.status.idle": "2024-06-19T10:46:23.204465Z",
     "shell.execute_reply": "2024-06-19T10:46:23.204272Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.204253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dipression - 1\n",
    "\n",
    "tweet_list = []\n",
    "for i in df['tweets']:\n",
    "    dictionary = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary:\n",
    "        tweet_list.append({'texts':j['tweet_content'],'labels':1})\n",
    "\n",
    "# Length of dipressed Tweets\n",
    "len(tweet_list)\n",
    "\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "# tweet_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-19T10:46:23.206061Z",
     "iopub.status.idle": "2024-06-19T10:46:23.206403Z",
     "shell.execute_reply": "2024-06-19T10:46:23.206256Z",
     "shell.execute_reply.started": "2024-06-19T10:46:23.206242Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df_2 = pd.read_csv('/kaggle/working/normalUserConvert.csv')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "str_dict = df_2['tweets'][0]\n",
    "dictionary_2 = ast.literal_eval(str_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:52:51.330909Z",
     "iopub.status.busy": "2024-06-19T10:52:51.330074Z",
     "iopub.status.idle": "2024-06-19T10:53:02.880664Z",
     "shell.execute_reply": "2024-06-19T10:53:02.879640Z",
     "shell.execute_reply.started": "2024-06-19T10:52:51.330878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nickname</th>\n",
       "      <th>gender</th>\n",
       "      <th>profile</th>\n",
       "      <th>birthday</th>\n",
       "      <th>num_of_follower</th>\n",
       "      <th>num_of_following</th>\n",
       "      <th>all_tweet_count</th>\n",
       "      <th>original_tweet_count</th>\n",
       "      <th>repost_tweet_count</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>多吃一点小糖豆</td>\n",
       "      <td>女</td>\n",
       "      <td>给自己起个小名叫快快</td>\n",
       "      <td>巨蟹座</td>\n",
       "      <td>530</td>\n",
       "      <td>380</td>\n",
       "      <td>368</td>\n",
       "      <td>28</td>\n",
       "      <td>70</td>\n",
       "      <td>[{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>铁拳会说话</td>\n",
       "      <td>女</td>\n",
       "      <td>漂泊的灵魂</td>\n",
       "      <td>射手座</td>\n",
       "      <td>137</td>\n",
       "      <td>235</td>\n",
       "      <td>154</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>梦想家脸庞eyti</td>\n",
       "      <td>男</td>\n",
       "      <td>在任何科学上的雏形，都有它双重的形象：胚胎时的丑恶，萌芽时的美丽。</td>\n",
       "      <td>无</td>\n",
       "      <td>75</td>\n",
       "      <td>117</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>[{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>小MM-Qun</td>\n",
       "      <td>女</td>\n",
       "      <td>Group</td>\n",
       "      <td>01-01</td>\n",
       "      <td>190</td>\n",
       "      <td>410</td>\n",
       "      <td>1834</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '母親節 準備好了麼～', 'posting_time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Outofcuriosity_</td>\n",
       "      <td>女</td>\n",
       "      <td>This is a blog only written original contents....</td>\n",
       "      <td>无</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         nickname gender  \\\n",
       "0      1          多吃一点小糖豆      女   \n",
       "1      1            铁拳会说话      女   \n",
       "2      0        梦想家脸庞eyti      男   \n",
       "3      0          小MM-Qun      女   \n",
       "4      1  Outofcuriosity_      女   \n",
       "\n",
       "                                             profile birthday  \\\n",
       "0                                         给自己起个小名叫快快      巨蟹座   \n",
       "1                                              漂泊的灵魂      射手座   \n",
       "2                  在任何科学上的雏形，都有它双重的形象：胚胎时的丑恶，萌芽时的美丽。        无   \n",
       "3                                              Group    01-01   \n",
       "4  This is a blog only written original contents....        无   \n",
       "\n",
       "   num_of_follower  num_of_following  all_tweet_count  original_tweet_count  \\\n",
       "0              530               380              368                    28   \n",
       "1              137               235              154                    20   \n",
       "2               75               117               87                    75   \n",
       "3              190               410             1834                    99   \n",
       "4                1                 9                6                    12   \n",
       "\n",
       "   repost_tweet_count                                             tweets  \n",
       "0                  70  [{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...  \n",
       "1                   2  [{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...  \n",
       "2                  10  [{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...  \n",
       "3                   1  [{'tweet_content': '母親節 準備好了麼～', 'posting_time...  \n",
       "4                   0  [{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_normal = pd.read_csv('/kaggle/working/normalUserConvert.csv')\n",
    "df_deep = pd.read_csv('/kaggle/working/depressUserConvertFile.csv')\n",
    "\n",
    "# Randomly select 3000 rows from each DataFrame\n",
    "df_normal = df_normal.sample(n=3000, random_state=42)\n",
    "df_deep = df_deep.sample(n=3000, random_state=42)\n",
    "\n",
    "# Concatenate the sampled DataFrames\n",
    "final_df = pd.concat([df_normal, df_deep])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "final_df = shuffle(final_df, random_state=42)\n",
    "\n",
    "# Reset index after shuffling\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the first few rows of the shuffled DataFrame\n",
    "# print(df_combined_shuffled.head())\n",
    "final_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T10:53:07.913033Z",
     "iopub.status.busy": "2024-06-19T10:53:07.912632Z",
     "iopub.status.idle": "2024-06-19T10:53:07.927515Z",
     "shell.execute_reply": "2024-06-19T10:53:07.926349Z",
     "shell.execute_reply.started": "2024-06-19T10:53:07.913004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'tweet_content': '母親節 準備好了麼～', 'posting_time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             tweets\n",
       "0      1  [{'tweet_content': '年纪越大越想做一个小朋友', 'posting_ti...\n",
       "1      1  [{'tweet_content': '他喜欢看知音身为 一个堂堂七尺男儿却喜欢看知音一定是...\n",
       "2      0  [{'tweet_content': '美丽是一步步来的上海隆鼻哪家整形医院好', 'pos...\n",
       "3      0  [{'tweet_content': '母親節 準備好了麼～', 'posting_time...\n",
       "4      1  [{'tweet_content': '惊了 我有点被这抗抑郁的药吓到了中午吃了这心情也太好..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df[['label','tweets']]\n",
    "\n",
    "print(len(final_df))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T11:00:19.241974Z",
     "iopub.status.busy": "2024-06-19T11:00:19.241558Z",
     "iopub.status.idle": "2024-06-19T12:20:41.626224Z",
     "shell.execute_reply": "2024-06-19T12:20:41.625239Z",
     "shell.execute_reply.started": "2024-06-19T11:00:19.241941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 38518\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be11aca7aa4c68ad02f26e30b9ab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Accuracy: 0.7554, Train Loss: 0.0707, Validation Accuracy: 0.7883, Validation Loss: 0.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Accuracy: 0.7562, Train Loss: 0.0697, Validation Accuracy: 0.7883, Validation Loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Accuracy: 0.7560, Train Loss: 0.0693, Validation Accuracy: 0.7883, Validation Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Accuracy: 0.7554, Train Loss: 0.0685, Validation Accuracy: 0.7883, Validation Loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Accuracy: 0.7550, Train Loss: 0.0669, Validation Accuracy: 0.7883, Validation Loss: 0.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Accuracy: 0.7535, Train Loss: 0.0629, Validation Accuracy: 0.7900, Validation Loss: 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Accuracy: 0.7704, Train Loss: 0.0595, Validation Accuracy: 0.8133, Validation Loss: 0.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Accuracy: 0.7946, Train Loss: 0.0576, Validation Accuracy: 0.8000, Validation Loss: 0.0564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Accuracy: 0.7979, Train Loss: 0.0558, Validation Accuracy: 0.8100, Validation Loss: 0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Accuracy: 0.8102, Train Loss: 0.0532, Validation Accuracy: 0.7833, Validation Loss: 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Accuracy: 0.8194, Train Loss: 0.0513, Validation Accuracy: 0.8183, Validation Loss: 0.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Accuracy: 0.8275, Train Loss: 0.0496, Validation Accuracy: 0.7767, Validation Loss: 0.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Accuracy: 0.8242, Train Loss: 0.0503, Validation Accuracy: 0.8100, Validation Loss: 0.0573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Accuracy: 0.8319, Train Loss: 0.0477, Validation Accuracy: 0.8017, Validation Loss: 0.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Accuracy: 0.8369, Train Loss: 0.0464, Validation Accuracy: 0.8017, Validation Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Accuracy: 0.8410, Train Loss: 0.0453, Validation Accuracy: 0.8100, Validation Loss: 0.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Accuracy: 0.8427, Train Loss: 0.0444, Validation Accuracy: 0.8100, Validation Loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Accuracy: 0.8521, Train Loss: 0.0432, Validation Accuracy: 0.8183, Validation Loss: 0.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Accuracy: 0.8565, Train Loss: 0.0417, Validation Accuracy: 0.7950, Validation Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Accuracy: 0.8577, Train Loss: 0.0407, Validation Accuracy: 0.8017, Validation Loss: 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Accuracy: 0.8690, Train Loss: 0.0399, Validation Accuracy: 0.7983, Validation Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Accuracy: 0.8725, Train Loss: 0.0376, Validation Accuracy: 0.7700, Validation Loss: 0.0837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Accuracy: 0.8802, Train Loss: 0.0362, Validation Accuracy: 0.7467, Validation Loss: 0.1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Accuracy: 0.8858, Train Loss: 0.0339, Validation Accuracy: 0.8083, Validation Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Accuracy: 0.8954, Train Loss: 0.0322, Validation Accuracy: 0.7783, Validation Loss: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Accuracy: 0.9056, Train Loss: 0.0307, Validation Accuracy: 0.7833, Validation Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Accuracy: 0.9117, Train Loss: 0.0276, Validation Accuracy: 0.7700, Validation Loss: 0.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Accuracy: 0.9231, Train Loss: 0.0243, Validation Accuracy: 0.7867, Validation Loss: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Accuracy: 0.9306, Train Loss: 0.0225, Validation Accuracy: 0.7867, Validation Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Accuracy: 0.9290, Train Loss: 0.0230, Validation Accuracy: 0.7600, Validation Loss: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Accuracy: 0.9329, Train Loss: 0.0209, Validation Accuracy: 0.7917, Validation Loss: 0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Accuracy: 0.9394, Train Loss: 0.0203, Validation Accuracy: 0.7600, Validation Loss: 0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Accuracy: 0.9390, Train Loss: 0.0190, Validation Accuracy: 0.7867, Validation Loss: 0.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Accuracy: 0.9387, Train Loss: 0.0194, Validation Accuracy: 0.7750, Validation Loss: 0.0936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Accuracy: 0.9492, Train Loss: 0.0168, Validation Accuracy: 0.8067, Validation Loss: 0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Accuracy: 0.9500, Train Loss: 0.0168, Validation Accuracy: 0.7567, Validation Loss: 0.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Accuracy: 0.9490, Train Loss: 0.0162, Validation Accuracy: 0.7550, Validation Loss: 0.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Accuracy: 0.9525, Train Loss: 0.0163, Validation Accuracy: 0.7633, Validation Loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Accuracy: 0.9581, Train Loss: 0.0144, Validation Accuracy: 0.7733, Validation Loss: 0.1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Accuracy: 0.9573, Train Loss: 0.0147, Validation Accuracy: 0.7850, Validation Loss: 0.1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Accuracy: 0.9604, Train Loss: 0.0128, Validation Accuracy: 0.7667, Validation Loss: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Accuracy: 0.9635, Train Loss: 0.0124, Validation Accuracy: 0.7833, Validation Loss: 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Accuracy: 0.9587, Train Loss: 0.0136, Validation Accuracy: 0.7850, Validation Loss: 0.1509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Accuracy: 0.9615, Train Loss: 0.0139, Validation Accuracy: 0.7233, Validation Loss: 0.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Accuracy: 0.9681, Train Loss: 0.0114, Validation Accuracy: 0.7617, Validation Loss: 0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Accuracy: 0.9640, Train Loss: 0.0121, Validation Accuracy: 0.7633, Validation Loss: 0.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Accuracy: 0.9675, Train Loss: 0.0121, Validation Accuracy: 0.7267, Validation Loss: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Accuracy: 0.9671, Train Loss: 0.0114, Validation Accuracy: 0.7783, Validation Loss: 0.1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Accuracy: 0.9744, Train Loss: 0.0092, Validation Accuracy: 0.7600, Validation Loss: 0.1369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Accuracy: 0.9775, Train Loss: 0.0084, Validation Accuracy: 0.7600, Validation Loss: 0.1425\n",
      "Test Accuracy: 0.7650, Test Loss: 0.1327\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Find the maximum length of the sequences in the dataset\n",
    "max_length = max(len(tokenizer.encode(text)) for text in final_df['tweets'])\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "# Set a maximum length for truncation\n",
    "max_length = min(max_length, 128)  # Limit the maximum length to 128 to conserve memory\n",
    "\n",
    "encoded_data = tokenizer(final_df['tweets'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(encoded_data['input_ids'], final_df['label'], test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Define the data loaders\n",
    "train_dataset = TensorDataset(train_texts, torch.tensor(train_labels))  # Ensure train_labels is converted to tensor\n",
    "val_dataset = TensorDataset(val_texts, torch.tensor(val_labels))  # Ensure val_labels is converted to tensor\n",
    "test_dataset = TensorDataset(test_texts, torch.tensor(test_labels))  # Ensure test_labels is converted to tensor\n",
    "\n",
    "# Define the model architecture\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define data loaders with smaller batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Reduced batch size to 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # Reduced batch size to 8\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # Reduced batch size to 8\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        train_acc = train_accuracy / train_total\n",
    "        progress_bar.set_postfix(train_acc=train_acc, train_loss=train_loss / train_total)\n",
    "    train_accuracy /= train_total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_accuracy += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "    val_accuracy /= val_total\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss / train_total:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss / val_total:.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_accuracy = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        test_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "test_accuracy /= test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss / test_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T12:20:41.628114Z",
     "iopub.status.busy": "2024-06-19T12:20:41.627827Z",
     "iopub.status.idle": "2024-06-19T12:20:41.832248Z",
     "shell.execute_reply": "2024-06-19T12:20:41.830644Z",
     "shell.execute_reply.started": "2024-06-19T12:20:41.628090Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLNetForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test)\n\u001b[1;32m      5\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLNetForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## XLEXNET 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/depressUserConvertFile.csv')\n",
    "\n",
    "str_dict = df['tweets'][1]\n",
    "dictionary = ast.literal_eval(str_dict)\n",
    "\n",
    "# dictionary\n",
    "print(dictionary)\n",
    "\n",
    "tweet_list_1 = []\n",
    "for i in df['tweets']:\n",
    "    dictionary = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary:\n",
    "        # tweet_list_1.append({'posting time':j['posting_time'],'labels':1})\n",
    "        tweet_list_1.append({'posting time':j['posting_time'],'orignal tweet':j['tweet_is_original'], 'labels':1})\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "print(len(tweet_list_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df_2 = pd.read_csv('/content/normalUserConvert.csv')\n",
    "\n",
    "str_dict = df_2['tweets'][0]\n",
    "dictionary_2 = ast.literal_eval(str_dict)\n",
    "\n",
    "tweet_list_normal = []\n",
    "for i in df['tweets']:\n",
    "    dictionary_2 = ast.literal_eval(str_dict)\n",
    "    # print(dictionary)\n",
    "    for j in dictionary_2:\n",
    "        tweet_list_normal.append({'Post Timing':j['posting_time'], 'orignal tweet':j['tweet_is_original'], 'labels':0})\n",
    "\n",
    "# Check & Display any dipressed tweet\n",
    "print(len(tweet_list_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Read the tweet_list_normal and tweet_list_dipressed into DataFrames\n",
    "df_normal_posting = pd.DataFrame(tweet_list_normal)\n",
    "df_deep_posting = pd.DataFrame(tweet_list_1)\n",
    "\n",
    "# Randomly select 3000 rows from each DataFrame\n",
    "df_normal_sampled = df_normal_posting.sample(n=3000, random_state=42)\n",
    "df_deep_sampled = df_deep_posting.sample(n=3000, random_state=42)\n",
    "\n",
    "# Concatenate the sampled DataFrames\n",
    "df_combined = pd.concat([df_normal_sampled, df_deep_sampled])\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "df_combined_shuffled = shuffle(df_combined, random_state=42)\n",
    "\n",
    "# Reset index after shuffling\n",
    "df_combined_shuffled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the first few rows of the shuffled DataFrame\n",
    "# print(df_combined_shuffled.head())\n",
    "df_combined_shuffled.head(10)\n",
    "\n",
    "final_df = df_combined_shuffled[['labels','Post Timing', 'orignal tweet']]\n",
    "\n",
    "print(len(final_df))\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'Post Timing' column to datetime format, handle parsing errors\n",
    "final_df['Post Timing'] = pd.to_datetime(final_df['Post Timing'], errors='coerce')\n",
    "\n",
    "# Remove rows with NaT values (parsing errors)\n",
    "final_df = final_df.dropna(subset=['Post Timing'])\n",
    "\n",
    "# Calculate features for each label separately\n",
    "social_features = {}\n",
    "\n",
    "# Iterate over unique labels\n",
    "for label in final_df['labels'].unique():\n",
    "    # Subset the DataFrame for the current label\n",
    "    subset_df = final_df[final_df['labels'] == label]\n",
    "\n",
    "    # Calculate features\n",
    "    # Proportion of original tweets\n",
    "    proportion_original = subset_df['orignal tweet'].value_counts(normalize=True)['True']\n",
    "\n",
    "    # Late-night posting (posts made between 11 PM and 6 AM)\n",
    "    late_night_posts = subset_df[(subset_df['Post Timing'].dt.hour >= 23) | (subset_df['Post Timing'].dt.hour <= 6)]\n",
    "    proportion_late_night_posts = len(late_night_posts) / len(subset_df)\n",
    "\n",
    "    # Posting frequency (per week)\n",
    "    posting_frequency_per_week = len(subset_df) / ((subset_df['Post Timing'].max() - subset_df['Post Timing'].min()).days / 7)\n",
    "\n",
    "    # Standard deviation of posting time\n",
    "    std_dev_posting_time = subset_df['Post Timing'].diff().dt.total_seconds().std()\n",
    "\n",
    "    # Store features for the current label\n",
    "    social_features[label] = {\n",
    "        'Proportion of Original Tweets': proportion_original,\n",
    "        'Proportion of Late-night Posts': proportion_late_night_posts,\n",
    "        'Posting Frequency (per week)': posting_frequency_per_week,\n",
    "        'Standard Deviation of Posting Time': std_dev_posting_time\n",
    "    }\n",
    "\n",
    "# Convert features dictionary to DataFrame\n",
    "social_features_df = pd.DataFrame.from_dict(social_features, orient='index')\n",
    "\n",
    "# Display the social features\n",
    "print(\"Social Behavior-based Features:\")\n",
    "print(social_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "# final_df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Combine 'Post Timing' and 'orignal tweet' features\n",
    "final_df['features'] = final_df['Post Timing'].astype(str) + ' ' + final_df['orignal tweet']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Find the maximum length of the sequences in the dataset\n",
    "max_length = max(len(tokenizer.encode(text)) for text in final_df['features'])\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "# Set a maximum length for truncation\n",
    "max_length = min(max_length, 64)  # Limit the maximum length to 128 to conserve memory\n",
    "\n",
    "encoded_data = tokenizer(final_df['features'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(encoded_data['input_ids'], final_df['labels'], test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(val_texts, val_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "test_labels = test_labels.to_numpy()\n",
    "\n",
    "# Define the data loaders\n",
    "train_dataset = TensorDataset(train_texts, torch.tensor(train_labels))  # Ensure train_labels is converted to tensor\n",
    "val_dataset = TensorDataset(val_texts, torch.tensor(val_labels))  # Ensure val_labels is converted to tensor\n",
    "test_dataset = TensorDataset(test_texts, torch.tensor(test_labels))  # Ensure test_labels is converted to tensor\n",
    "\n",
    "# Define the model architecture\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Define training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define data loaders with smaller batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size to 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced batch size to 8\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)  # Reduced batch size to 8\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        train_accuracy += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        train_acc = train_accuracy / train_total\n",
    "        progress_bar.set_postfix(train_acc=train_acc, train_loss=train_loss / train_total)\n",
    "    train_accuracy /= train_total\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_accuracy += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            val_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "    val_accuracy /= val_total\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss / train_total:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss / val_total:.4f}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_accuracy = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        test_accuracy += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        test_loss += torch.nn.functional.cross_entropy(logits, labels).item()\n",
    "test_accuracy /= test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss / test_total:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4493575,
     "sourceId": 7698496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
